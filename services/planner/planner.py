"""
Planner service: extract visualization intent and generate JSON specs.
Uses LLM to convert natural language to structured VisualizationSpec.
CRITICAL: Never executes code generated by LLM; only parses JSON.
"""

import json
import re
from typing import Any

from pydantic import ValidationError

from core.exceptions import SpecValidationError
from core.logging import get_logger
from services.ingest.parser import DatasetSchema
from services.llm.openai_client import OpenAIClient, UsageStats
from services.planner.spec_schema import (
    PlannerResponse,
    SpecError,
    VisualizationSpec,
)

logger = get_logger(__name__)


# Prompt templates
SYSTEM_PROMPT = """You are a data visualization expert assistant. Your job is to convert natural language requests into structured JSON visualization specifications.

You MUST respond with ONLY valid JSON matching the VisualizationSpec schema. Do not include any explanatory text before or after the JSON.

Supported chart types: line, bar, scatter, histogram, box, heatmap, pie, area

Supported aggregation functions: sum, mean, count, median, min, max, std

Supported filter operators: ==, !=, >, >=, <, <=, in, not in

Supported transformations: log, log10, sqrt, abs, diff, pct_change, rolling_mean

Example output format:
{
  "chart_type": "line",
  "x": "date",
  "y": "revenue",
  "aggregate": {"func": "sum", "group_by": ["region"]},
  "filters": [{"column": "year", "op": ">=", "value": 2020}],
  "transformations": [],
  "options": {
    "title": "Monthly Revenue by Region",
    "height": 600,
    "width": 900,
    "color": "region"
  },
  "explain": "Shows total revenue over time grouped by region"
}

If a column doesn't exist or is ambiguous, respond with:
{
  "error": "column_not_found",
  "message": "Column 'xyz' not found in dataset",
  "candidates": ["column1", "column2"]
}

CRITICAL RULES:
1. Output ONLY valid JSON, nothing else
2. Column names must exactly match those in the dataset schema
3. Never generate executable Python code
4. Use simple, safe column references only
5. If unsure, ask for clarification via error response
"""


def build_intent_prompt(
    user_query: str, schema: DatasetSchema, chat_history: list[dict[str, str]] | None = None
) -> str:
    """
    Build prompt for intent extraction.

    Args:
        user_query: User's natural language query
        schema: Dataset schema with column names and types
        chat_history: Optional previous chat messages for context

    Returns:
        Formatted prompt string
    """
    # Format schema info
    columns_info = []
    for col, dtype in schema.dtypes.items():
        missing = schema.missing_values.get(col, 0)
        columns_info.append(f"  - {col} ({dtype}) - {missing} missing values")

    schema_text = "\n".join(columns_info)

    # Format sample data
    sample_text = "Sample rows:\n"
    for i, row in enumerate(schema.sample_rows[:3], 1):
        sample_text += f"Row {i}: {row}\n"

    # Build context from chat history
    context_text = ""
    if chat_history and len(chat_history) > 0:
        context_text = "\n\nPrevious conversation context:\n"
        for msg in chat_history[-3:]:  # Last 3 messages
            role = msg.get("role", "user")
            content = msg.get("content", "")[:200]  # Truncate long messages
            context_text += f"{role}: {content}\n"

    prompt = f"""Dataset schema:
{schema_text}

{sample_text}
{context_text}

User request: {user_query}

Generate a VisualizationSpec JSON that fulfills this request. Remember: respond with ONLY the JSON, no extra text."""

    return prompt


def build_refinement_prompt(
    user_query: str,
    current_spec: VisualizationSpec,
    schema: DatasetSchema,
) -> str:
    """
    Build prompt for refining an existing spec.

    Args:
        user_query: User's modification request
        current_spec: Current visualization spec
        schema: Dataset schema

    Returns:
        Formatted prompt string
    """
    current_json = current_spec.model_dump_json(indent=2)

    prompt = f"""Current visualization spec:
{current_json}

Dataset columns: {', '.join(schema.columns)}

User wants to modify: {user_query}

Generate an updated VisualizationSpec JSON. Respond with ONLY the JSON."""

    return prompt


class VisualizationPlanner:
    """
    Generate visualization specifications from natural language using LLM.
    Ensures all outputs are valid JSON specs, never executable code.
    """

    def __init__(self, llm_client: OpenAIClient | None = None) -> None:
        """
        Initialize planner.

        Args:
            llm_client: Optional LLM client (creates new one if not provided)
        """
        self.llm_client = llm_client or OpenAIClient()
        logger.info("Visualization planner initialized")

    def _extract_json_from_response(self, response: str) -> dict[str, Any]:
        """
        Extract JSON from LLM response.
        LLMs sometimes wrap JSON in markdown or add extra text.

        Args:
            response: Raw LLM response

        Returns:
            Parsed JSON dictionary

        Raises:
            SpecValidationError: If JSON extraction fails
        """
        # Try direct parse first
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            pass

        # Try to find JSON in code blocks
        json_pattern = r"```(?:json)?\s*(\{.*?\})\s*```"
        matches = re.findall(json_pattern, response, re.DOTALL)
        if matches:
            try:
                return json.loads(matches[0])
            except json.JSONDecodeError:
                pass

        # Try to find any JSON object
        json_pattern = r"\{.*\}"
        matches = re.findall(json_pattern, response, re.DOTALL)
        if matches:
            for match in matches:
                try:
                    return json.loads(match)
                except json.JSONDecodeError:
                    continue

        raise SpecValidationError(
            "Could not extract valid JSON from LLM response",
            details={"response": response[:500]},
        )

    def plan(
        self,
        user_query: str,
        schema: DatasetSchema,
        chat_history: list[dict[str, str]] | None = None,
    ) -> tuple[PlannerResponse, UsageStats]:
        """
        Generate visualization spec from user query.

        Args:
            user_query: Natural language visualization request
            schema: Dataset schema
            chat_history: Optional chat history for context

        Returns:
            Tuple of (PlannerResponse, UsageStats)
        """
        logger.info("Planning visualization", query=user_query[:100])

        # Build prompt
        prompt = build_intent_prompt(user_query, schema, chat_history)

        # Call LLM
        try:
            response, usage = self.llm_client.complete(
                prompt=prompt, system_prompt=SYSTEM_PROMPT, temperature=0.1
            )

            logger.debug("LLM response received", response_length=len(response))

            # Extract and parse JSON
            json_data = self._extract_json_from_response(response)

            # Check for error response
            if "error" in json_data:
                error = SpecError(**json_data)
                return PlannerResponse(spec=None, error=error, raw_llm_output=response), usage

            # Validate spec
            spec = VisualizationSpec(**json_data)

            logger.info(
                "Visualization spec generated",
                chart_type=spec.chart_type,
                x=spec.x,
                y=spec.y,
                has_aggregate=spec.aggregate is not None,
                has_filters=len(spec.filters) > 0,
            )

            return PlannerResponse(spec=spec, error=None, raw_llm_output=response), usage

        except ValidationError as e:
            logger.error("Spec validation failed", error=str(e))
            error = SpecError(
                error="validation_error",
                message=f"Generated spec failed validation: {str(e)}",
                candidates=[],
            )
            return PlannerResponse(spec=None, error=error, raw_llm_output=response), usage

        except Exception as e:
            logger.error("Planning failed", error=str(e), error_type=type(e).__name__)
            error = SpecError(
                error="planning_error", message=f"Failed to generate spec: {str(e)}", candidates=[]
            )
            return (
                PlannerResponse(spec=None, error=error, raw_llm_output=""),
                UsageStats(
                    prompt_tokens=0,
                    completion_tokens=0,
                    total_tokens=0,
                    model=self.llm_client.model,
                    duration_seconds=0.0,
                ),
            )

    def refine(
        self,
        user_query: str,
        current_spec: VisualizationSpec,
        schema: DatasetSchema,
    ) -> tuple[PlannerResponse, UsageStats]:
        """
        Refine an existing visualization spec based on user feedback.

        Args:
            user_query: User's modification request
            current_spec: Current visualization spec
            schema: Dataset schema

        Returns:
            Tuple of (PlannerResponse, UsageStats)
        """
        logger.info("Refining visualization", query=user_query[:100])

        # Build refinement prompt
        prompt = build_refinement_prompt(user_query, current_spec, schema)

        # Call LLM
        try:
            response, usage = self.llm_client.complete(
                prompt=prompt, system_prompt=SYSTEM_PROMPT, temperature=0.1
            )

            # Extract and parse JSON
            json_data = self._extract_json_from_response(response)

            # Validate spec
            spec = VisualizationSpec(**json_data)

            logger.info("Visualization spec refined", chart_type=spec.chart_type)

            return PlannerResponse(spec=spec, error=None, raw_llm_output=response), usage

        except Exception as e:
            logger.error("Refinement failed", error=str(e))
            error = SpecError(
                error="refinement_error",
                message=f"Failed to refine spec: {str(e)}",
                candidates=[],
            )
            return (
                PlannerResponse(spec=None, error=error, raw_llm_output=""),
                UsageStats(
                    prompt_tokens=0,
                    completion_tokens=0,
                    total_tokens=0,
                    model=self.llm_client.model,
                    duration_seconds=0.0,
                ),
            )
