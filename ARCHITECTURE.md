# Architecture

This document describes how I designed and built the AI Data Visualization Agent. I'm writing this in first person to share my architectural decisions and design philosophy.

---

## üéØ Design Philosophy

When I set out to build this system, I had three core principles in mind:

1. **Security First**: I never wanted to execute arbitrary code generated by an LLM. Everything must go through validated, type-safe specifications.
2. **Modularity**: I designed each component to be independently testable and replaceable.
3. **Production-Ready**: I built this for real-world use, with logging, error handling, CI/CD, and observability from day one.

---

## üèõÔ∏è System Overview

I structured the application as a pipeline with clear separation of concerns:

```
User Question ‚Üí Ingestion ‚Üí Profiling ‚Üí Planning ‚Üí Rendering ‚Üí Display
                    ‚Üì                      ‚Üì           ‚Üì
                 Memory ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ LLM Client ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### High-Level Flow

1. **User uploads a file** ‚Üí I parse and validate it (CSV/XLSX)
2. **I profile the data** ‚Üí Extract schema, statistics, and suggest charts
3. **User asks a question** ‚Üí I store it in memory
4. **I send context to LLM** ‚Üí Dataset schema + chat history ‚Üí OpenAI
5. **LLM returns JSON spec** ‚Üí I validate it with Pydantic (no code execution!)
6. **I render the chart** ‚Üí Map spec to safe Pandas + Plotly operations
7. **I display result** ‚Üí Show interactive chart + store in memory

---

## üì¶ Module Breakdown

### Core Modules

#### `core/config.py`
I centralized all configuration using Pydantic Settings. This reads from `.env` files and validates everything at startup. I specifically:
- Enforce API key presence (never hardcoded)
- Set sane defaults for all options
- Provide type-safe access to settings everywhere

#### `core/logging.py`
I use `structlog` for structured JSON logging. Key features:
- Automatic censoring of sensitive data (API keys, passwords)
- Correlation IDs for request tracing
- Different formats for dev (console) vs prod (JSON)
- Integration with standard library logging

#### `core/exceptions.py`
I defined custom exception hierarchy for clear error handling:
- `AIVizException` - Base for all app errors
- `FileUploadError`, `FileParsingError` - Data ingestion errors
- `LLMError`, `LLMRateLimitError` - API errors
- `SpecValidationError`, `RenderError` - Processing errors

---

### Services Layer

#### `services/ingest/parser.py`

I built this to safely parse uploaded files:

**Key Design Decisions:**
- **Security**: I sanitize filenames to prevent directory traversal
- **Validation**: I enforce file size limits (configurable, default 50MB)
- **Type Inference**: I automatically detect CSV delimiters and data types
- **Error Handling**: I gracefully handle encoding issues (UTF-8 ‚Üí latin1 fallback)

**Why I Did It This Way:**
- Users upload untrusted files. I validate everything before processing.
- CSV files are notoriously inconsistent. I handle edge cases (different delimiters, encodings).
- I extract schema immediately for LLM context.

#### `services/profile/profiler.py`

I profile datasets to provide context and recommendations:

**What I Profile:**
- Column types (numeric, categorical, datetime, boolean)
- Summary statistics (mean, median, std, min, max)
- Missing value analysis
- Value distributions and cardinality

**Auto-Recommendations:**
I generate suggested visualizations based on data characteristics:
- Histogram for numeric columns
- Bar chart for categorical columns
- Time series for datetime + numeric
- Scatter for multiple numerics
- Box plots for numeric grouped by categorical

**Design Rationale:**
- I want to help users who don't know what to ask for
- Profiling provides rich context for the LLM
- Pre-computed stats speed up rendering

#### `services/llm/openai_client.py`

I wrapped the OpenAI API with production-grade features:

**Features:**
- **Retry Logic**: I use `tenacity` for exponential backoff on rate limits/timeouts
- **Usage Tracking**: I log token counts for cost monitoring
- **Rate Limiting**: I enforce minimum delay between calls
- **Error Handling**: I convert API errors to our exception types

**Security:**
- I never log API keys
- I never log full API responses (may contain sensitive data)
- API key loaded from environment only

**Why This Design:**
- OpenAI API can be unreliable (rate limits, timeouts)
- I need visibility into usage for cost control
- I want to abstract away API details from the rest of the system

#### `services/planner/spec_schema.py` & `planner.py`

**This is the heart of my security model.**

I defined a strict JSON schema (`VisualizationSpec`) that the LLM must output:

```python
{
  "chart_type": "line | bar | scatter | ...",
  "x": "column_name",
  "y": "column_name",
  "aggregate": {"func": "sum|mean|...", "group_by": [...]},
  "filters": [{"column": "...", "op": ">=", "value": ...}],
  "transformations": [{"op": "log|sqrt|...", "column": "..."}],
  "options": {"title": "...", "color": "...", ...},
  "explain": "Human-readable description"
}
```

**Critical Design Decisions:**

1. **No Code Execution**: I NEVER accept Python code from the LLM. Only validated JSON.
2. **Enum-Based Operations**: I restrict operations to a safe whitelist (no `eval`, `exec`, `__import__`).
3. **Pydantic Validation**: I reject any spec that doesn't match the schema.
4. **Column Name Sanitization**: I strip dangerous characters (`eval`, `exec`, `;`, etc.).

**Prompt Engineering:**

I crafted prompts that:
- Include dataset schema (columns, types, sample rows)
- Provide examples of valid specs
- Request JSON-only output (no extra text)
- Use low temperature (0.1) for consistency
- Include recent chat history for context

**JSON Extraction:**

I handle LLMs that wrap JSON in markdown or add extra text:
1. Try direct JSON parse
2. Extract from markdown code blocks
3. Find JSON object with regex
4. Raise error if no valid JSON found

**Why This Approach:**

- **Security**: Arbitrary code execution is the #1 risk with LLM systems. I eliminate it entirely.
- **Reliability**: Structured outputs are much more reliable than parsing code.
- **Debuggability**: I can validate, log, and inspect specs easily.
- **Extensibility**: Adding new chart types or operations is just adding enums.

#### `services/renderer/plotly_renderer.py` & `seaborn_renderer.py`

I implemented safe rendering with two backends:

**PlotlyRenderer (Primary):**

I map JSON specs to Plotly figures using ONLY safe operations:

- **Filters**: I use Pandas boolean indexing (`df[condition]`)
- **Transformations**: I implement a whitelist of numpy functions (`log`, `sqrt`, `abs`, etc.)
- **Aggregations**: I use Pandas `groupby().agg()` with function names (not lambdas)

**Safety Guarantees:**
- No `eval()` or `exec()`
- No dynamic code generation
- No user-supplied function objects
- All operations are from a fixed set

**SeabornRenderer (Fallback):**

I reuse PlotlyRenderer's transformation logic but render with Matplotlib/Seaborn. This provides:
- Static fallback if Plotly fails
- PNG exports
- Simpler charts for quick visualizations

**Why Two Renderers:**

- Plotly is primary (interactive, beautiful, exportable to HTML)
- Seaborn is fallback (static, simpler, sometimes more stable)
- Separation allows easy addition of more renderers

#### `services/memory/sql_memory.py` & `vector_memory.py`

I implemented two-tier memory:

**SQLite Memory (Primary):**

I store **append-only** chat history:
- Session-based message storage
- No overwrites (audit trail)
- Simple text search
- Embeddings field for future vector search

**Key Design:**
- `messages` table: session_id, timestamp, role, content, metadata, embedding
- Indexed on session_id for fast retrieval
- JSON metadata for extensibility

**Vector Memory (Optional):**

I provide a pluggable vector store interface:
- ChromaDB implementation included
- Semantic search over chat history
- Extensible to other vector DBs (FAISS, Pinecone, etc.)

**Why Dual Memory:**

- SQLite is simple, reliable, zero-config
- Vector search enables RAG and semantic context retrieval
- Pluggable design allows swapping implementations

---

### Frontend (Streamlit)

#### `streamlit_frontend/app.py`

I built the main app with Streamlit for rapid development and reactivity.

**Session Management:**

I use Streamlit session state to maintain:
- Unique session ID (UUID)
- Memory backend instance
- LLM client and planner
- Current dataset and schema
- Current spec and figure

**UI Layout:**

I chose a two-column layout:
- **Left**: Chat interface with message history
- **Right**: Visualization canvas with export controls

**Interaction Flow:**

1. User uploads file ‚Üí I parse, profile, show schema
2. User types question ‚Üí I add to memory
3. I query LLM with context ‚Üí Generate spec
4. I render chart ‚Üí Display result
5. I store response in memory

**Why Streamlit:**

- Fast prototyping (built in days, not weeks)
- Reactive updates (state management handled automatically)
- Built-in components (file upload, chat, charts)
- Easy deployment
- Pythonic (no frontend framework needed)

#### `streamlit_frontend/components/`

I split the UI into reusable components:

- **`chat.py`**: Message rendering with role-based styling
- **`uploader.py`**: File upload with validation and schema preview
- **`plot_controls.py`**: Manual editing, JSON editor, export buttons

**Design Rationale:**

- Modularity: Each component is independently testable
- Reusability: Can use components across different views
- Clarity: Small files with single responsibilities

---

## üîÑ Data Flow

Let me walk through a complete user interaction:

### Example: "Show monthly revenue by region"

1. **User uploads** `sales_data.csv` (10k rows, 5 columns)
2. **Parser** (`ingest/parser.py`):
   - Validates file size and extension
   - Infers CSV delimiter (`,`)
   - Parses into Pandas DataFrame
   - Extracts schema: `{date, region, product, revenue, units}`
3. **Profiler** (`profile/profiler.py`):
   - Classifies columns: `date` (object), `region` (categorical), `revenue` (numeric)
   - Computes statistics: mean revenue = $15,234, etc.
   - Recommends: "Line chart of revenue over time"
4. **User asks**: "Show monthly revenue by region"
5. **Memory** (`memory/sql_memory.py`):
   - Stores user message with session_id and timestamp
6. **Planner** (`planner/planner.py`):
   - Builds prompt with schema + sample rows + chat history
   - Calls OpenAI API with system prompt
   - **LLM returns**:
     ```json
     {
       "chart_type": "line",
       "x": "date",
       "y": "revenue",
       "aggregate": {"func": "sum", "group_by": ["region"]},
       "options": {"color": "region", "title": "Monthly Revenue by Region"},
       "explain": "Shows total revenue over time for each region"
     }
     ```
   - Validates JSON against Pydantic schema
7. **Renderer** (`renderer/plotly_renderer.py`):
   - Applies aggregation: `df.groupby('region')['revenue'].sum()`
   - Creates Plotly line chart with color by region
   - Returns interactive figure
8. **Frontend** displays chart
9. **Memory** stores assistant response with metadata (tokens used, duration)

---

## üîê Security Architecture

I designed multiple layers of security:

### Layer 1: Input Validation

- Filename sanitization (prevent directory traversal)
- File size limits (prevent DoS)
- File type validation (only CSV/XLSX)
- Column name sanitization (strip dangerous chars)

### Layer 2: No Code Execution

- LLM outputs are JSON specs ONLY
- I parse and validate with Pydantic
- I reject any spec with invalid operations
- No `eval()`, `exec()`, `compile()`, `__import__()` anywhere in codebase

### Layer 3: Safe Operations

- Filters use boolean indexing (no query strings)
- Transformations use whitelisted numpy functions
- Aggregations use predefined function names
- All data operations are pure Pandas (no custom code)

### Layer 4: Secrets Management

- API keys from environment variables only
- Never logged or printed
- Censored in structured logs
- Not included in error messages

### Layer 5: Container Security

- Non-root user in Docker
- Minimal base image (Python slim)
- No unnecessary packages
- Health checks enabled

---

## üìä Observability

I built in observability from the start:

### Logging

- Structured JSON logs (production)
- Human-readable console (development)
- Correlation IDs for tracing
- Sensitive data redaction
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL

### Metrics (Future)

I designed hooks for:
- LLM call counters
- Token usage tracking
- Render success/failure rates
- Response time histograms
- Error rate tracking

### Health Checks

- Docker health check (`/_stcore/health`)
- Liveness probe ready for k8s

---

## üß™ Testing Strategy

I implemented comprehensive testing:

### Unit Tests

- **Ingestion**: CSV parsing, validation, schema extraction
- **Planner**: JSON parsing, spec validation, LLM mocking
- **Renderer**: Chart rendering, filter application, aggregation

### Integration Tests

- End-to-end flows (upload ‚Üí query ‚Üí render)
- Memory persistence
- Error handling paths

### Mocking Strategy

- I mock external dependencies (OpenAI API)
- I use fixtures for test data
- I test both happy and error paths

---

## üöÄ Deployment

I designed for multiple deployment options:

### Local Development

```bash
uv run streamlit run streamlit_frontend/app.py
```

### Docker

```bash
docker-compose up
```

### Kubernetes (Future)

I designed with k8s in mind:
- Non-root container
- Health checks
- ConfigMaps for configuration
- Secrets for API keys
- Horizontal scaling ready (stateless design)

---

## üîÆ Future Enhancements

I've identified these architectural improvements:

1. **Caching Layer**: Cache LLM responses for identical queries
2. **Async Processing**: Use async/await for I/O operations
3. **Message Queue**: Offload heavy rendering to workers
4. **Multi-tenancy**: Add user authentication and workspace isolation
5. **Plugin System**: Allow custom chart types and data sources
6. **Streaming**: Stream LLM responses for faster perceived performance

---

## üéì Lessons Learned

Key insights from building this:

1. **Security Can't Be An Afterthought**: I designed the spec-based approach from the start to avoid code execution risks.
2. **Observability Is Essential**: Structured logging saved me hours of debugging.
3. **Type Safety Matters**: Pydantic caught countless bugs before they reached production.
4. **Test Early, Test Often**: Tests gave me confidence to refactor aggressively.
5. **Documentation Is For Future Me**: I wrote this doc knowing I'd forget my own decisions in 6 months.

---

**This architecture represents my vision for production-ready AI applications: secure, observable, maintainable, and built to last.**
